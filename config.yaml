# ==========================================
# BENCHMARK CONFIGURATION
# ==========================================

pipeline:
  run_download: true # Set to true to download listed models (only needed the first time)
  run_benchmark: true # set to generate answers for listed models over all questions
  run_evaluation: true # set to true to compute metrics and generate summary CSV

download:
  models:
  - "qwen-2.5-3b"
  - "llama-3.2-1b"
  - "phi-3-mini"
  - "gemma-2-2b"
  token: null # Add Hugging Face token if you want to download private models or avoid rate limits (optional)

benchmark:
  models:
  - "qwen-2.5-3b"
  - "llama-3.2-1b"
  - "phi-3-mini"
  - "gemma-2-2b"
  limit: 5 # Set to null tu run on all questions

evaluation:
  results_dir: "results"
  output_csv: "benchmark_summary.csv"
