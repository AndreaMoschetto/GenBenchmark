****# ğŸš€ GenBenchmark: A RAG Benchmark Pipeline over Generation Models

## ğŸ¯ Project Purpose
This project implements a complete pipeline to evaluate the performance of various Large Language Models (LLMs) within a **RAG (Retrieval-Augmented Generation)** architecture. 
The goal is to test the ability of open-weights models (such as Llama-3, Phi-3, Qwen-2.5, Gemma-2) to answer complex questions relying exclusively on text passages dynamically retrieved by the system.

The benchmark uses a subset (*well-formed answers*) of the famous **MS MARCO** dataset and evaluates the generated answers using multiple metrics:
1. **MRR (Mean Reciprocal Rank)**: Evaluates the quality of the retriever (FAISS + Sentence Transformers).
2. **ROUGE-L and BLEU**: Evaluate the lexical overlap between the generated text and the *Ground Truth*.
3. **Semantic Similarity**: Evaluates semantic correctness using spatial embeddings (Cosine Similarity).

---

## ğŸ“‚ Project Structure

The project is engineered to be modular and easily reproducible. All code comments or readme.md have to be in English [cite: 2026-02-26].

```text
GENBENCHMARK/
â”œâ”€â”€ data/                      # JSON dataset, FAISS index (.faiss), and metadata (.pkl)
â”œâ”€â”€ models_cache/              # Local cache folder for model weights downloaded from Hugging Face
â”œâ”€â”€ rag/                       # Core logic of the RAG architecture
â”‚   â”œâ”€â”€ generator.py           # LocalGenerator class: LLM loading and inference management
â”‚   â”œâ”€â”€ ingestion.py           # Creation and saving of the FAISS vector index
â”‚   â””â”€â”€ retriever.py           # FaissRetriever class: top-K context extraction
â”œâ”€â”€ results/                   # Raw JSON results generated by individual models
â”œâ”€â”€ utils/                     # Support scripts
â”‚   â”œâ”€â”€ download_dataset.py    # Download and pre-processing of MS MARCO
â”‚   â””â”€â”€ download_models.py     # Download model weights from HF
â”œâ”€â”€ .env                       # File for secret environment variables (e.g., HF_TOKEN)
â”œâ”€â”€ benchmark.py               # Answer generation script over RAG
â”œâ”€â”€ benchmark_summary.csv      # Final output with aggregated metrics table
â”œâ”€â”€ config.yaml                # Experiment orchestration file
â”œâ”€â”€ constants.py               # Global paths definition
â”œâ”€â”€ evaluate.py                # Metrics calculation script (ROUGE, BLEU, MRR, Sim)
â””â”€â”€ main.py                    # Main orchestrator (runs everything reading config.yaml)

```

---

## ğŸš€ Getting Started (Setup)

### 1. Environment Preparation

This project relies on specific library versions (e.g., PyTorch 2.2.0 and Transformers 4.45.2) to ensure compatibility with HPC clusters and specific model architectures.

We highly recommend using **Conda** to manage dependencies, as it natively handles complex C++ libraries like FAISS much better than standard pip.

You can automatically recreate the exact environment using the provided `environment.yml` file. Run the following commands in your terminal at the root of the project:

```bash
# Create the conda environment from the yaml file
conda env create -f environment.yml

# Activate the newly created environment
conda activate rag-benchmark

```

### 2. Secrets Configuration (`.env`)

To download certain gated models (like Llama-3 or Gemma-2), a Hugging Face token is required.

1. Copy or rename the `.env.example` file to `.env`:

```bash
cp .env.example .env

```

2. Insert your token inside the `.env` file:

```env
HF_TOKEN=hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

```

*(Note: The `.env` file is ignored by Git for security reasons).*

---

## âš™ï¸ Configuration: `constants.py` vs `config.yaml`

The project uses two distinct configuration files. Understanding when to use which is crucial:

* ğŸ› ï¸ **`constants.py` (System Paths):** Modify this file **only the first time** if you want to change the project's folder structure (e.g., if you prefer to save data on an external hard drive or an HPC *scratch* partition). It contains constants like `DATA_PATH`, `MODELS_DIR`, etc.
* ğŸ§  **`config.yaml` (Experiment Brain):** Modify this file **before every run**. This is where you define the experiment: which models to download, which to test in the benchmark, how many questions to process (`limit`), and which pipeline phases to skip (via the `true/false` flags under `pipeline:`).

---

## ğŸƒâ€â™‚ï¸ Running the Benchmark

### The Recommended Method (YAML Orchestrator)

The best and most reproducible way to launch the benchmark (especially on clusters) is to use the `main.py` script, which will automatically read your configuration from `config.yaml`.

1. Open `config.yaml` and set the models and phases to execute (e.g., set only `run_benchmark` and `run_evaluation` to `true`).
2. Start the pipeline:

```bash
python main.py --config config.yaml

```

*(If working on a cluster, run this command inside your submission script `run-benchmark.sh`).*

---

<details>
<summary><strong>ğŸ› ï¸ Manual Execution (Single Scripts)</strong></summary>

If you need to debug or want to run individual modules step-by-step by passing command-line parameters, you can use the dedicated scripts directly:

**0. Dataset Download and Pre-processing:**

```bash
python utils/download_dataset.py

```

**1. Models Download:**

```bash
python utils/download_models.py --models llama-3.2-1b qwen-2.5-3b --token YOUR_TOKEN_HERE

```

**2. FAISS Ingestion:**
*(This operation is integrated into the data preparation, but if you scripted `ingestion.py` separately, you can call it here).*

**3. Benchmark Start (Generation):**

```bash
python benchmark.py --models llama-3.2-1b qwen-2.5-3b --limit 5

```

**4. Metrics Calculation:**

```bash
python evaluate.py --dir results --output benchmark_summary.csv

```

</details>

---

## ğŸ“Š Final Benchmark Results and Analysis

The final evaluation isolates the retrieval performance from the generative performance to provide a comprehensive analysis of the RAG pipeline.

### Retrieval Performance

The retrieval phase is model-agnostic and relies on the `all-MiniLM-L6-v2` embedding model coupled with a FAISS index.

* **Global MRR (Mean Reciprocal Rank): `0.5407`**
* *Interpretation:* An MRR of ~0.54 indicates that, on average, the correct supporting passage (annotated as `is_selected: 1` in the MS MARCO dataset) is positioned between rank 1 and rank 2 in the retrieved context. This demonstrates a highly effective retrieval component, ensuring the generative models receive high-quality grounding information.

### Generative Models Performance

The following table presents the metrics for the generated answers, evaluated against both the *Well-Formed (WF)* answers and the *Original (Orig)* answers provided by the MS MARCO dataset.

| Model | WF ROUGE-L | WF BLEU | WF Similarity | Orig ROUGE-L | Orig BLEU | Orig Similarity |
| --- | --- | --- | --- | --- | --- | --- |
| **Qwen-2.5-3b** | 0.3452 | 0.1166 | **0.8031** | 0.2340 | 0.0675 | 0.5338 |
| **Phi-3-mini** | **0.3747** | **0.1485** | 0.7409 | 0.2515 | 0.0779 | 0.5062 |
| **Gemma-2-2b** | 0.3452 | 0.1283 | 0.6315 | **0.3855** | **0.1589** | **0.5930** |
| **Llama-3.2-1b** | 0.3413 | 0.1224 | 0.6389 | 0.2310 | 0.0703 | 0.4510 |

### Insights and Interpretations

1. **Qwen-2.5-3b (The Semantic Leader):** While it does not hold the highest n-gram overlap (ROUGE/BLEU), it completely dominates the **Well-Formed Semantic Similarity (0.8031)**. This indicates that Qwen profoundly understands the context and accurately grasps the core concepts of the well-formed answers, rephrasing them with its own highly capable vocabulary rather than memorizing the exact wording.
2. **Phi-3-mini (The Instruction Follower):** Phi-3 achieves the highest strict lexical overlap (**WF ROUGE-L: 0.3747**, **WF BLEU: 0.1485**) for the well-formed answers. It balances semantic understanding with strict instruction-following, generating text that closely mirrors the syntactical structure expected by human annotators.
3. **Gemma-2-2b (The Extractive Summarizer):** Gemma exhibits a highly unique behavior: it performs significantly better on the *Original* answers than on the *Well-Formed* ones. Scoring the highest **Orig ROUGE-L (0.3855)** means the model acts extractively, preferring to "copy and paste" direct segments from the retrieved context rather than synthesizing a smooth, conversational response.
4. **Llama-3.2-1b (The Efficient Baseline):** Despite being less than half the size of Qwen and Phi-3, Llama 1B provides an incredibly competitive baseline. Its well-formed scores remain heavily in line with the larger models, proving it to be a highly efficient choice for resource-constrained RAG deployments.
