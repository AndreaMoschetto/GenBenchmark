# ğŸš€ GenBenchmark: A RAG Benchmark Pipeline over Generation **models**

## ğŸ¯ Project Purpose
This project implements a complete pipeline to evaluate the performance of various Large Language Models (LLMs) within a **RAG (Retrieval-Augmented Generation)** architecture. 
The goal is to test the ability of open-weights models (such as Llama-3, Phi-3, Qwen-2.5, Gemma-2) to answer complex questions relying exclusively on text passages dynamically retrieved by the system.

The benchmark uses a subset (*well-formed answers*) of the famous **MS MARCO** dataset and evaluates the generated answers using multiple metrics:
1. **MRR (Mean Reciprocal Rank)**: Evaluates the quality of the retriever (FAISS + Sentence Transformers).
2. **ROUGE-L and BLEU**: Evaluate the lexical overlap between the generated text and the *Ground Truth*.
3. **Semantic Similarity**: Evaluates semantic correctness using spatial embeddings (Cosine Similarity).

---

## ğŸ“‚ Project Structure

The project is engineered to be modular and easily reproducible:

```text
GENBENCHMARK/
â”œâ”€â”€ data/                      # JSON dataset, FAISS index (.faiss), and metadata (.pkl)
â”œâ”€â”€ models_cache/              # Local cache folder for model weights downloaded from Hugging Face
â”œâ”€â”€ rag/                       # Core logic of the RAG architecture
â”‚   â”œâ”€â”€ generator.py           # LocalGenerator class: LLM loading and inference management
â”‚   â”œâ”€â”€ ingestion.py           # Creation and saving of the FAISS vector index
â”‚   â””â”€â”€ retriever.py           # FaissRetriever class: top-K context extraction
â”œâ”€â”€ results/                   # Raw JSON results generated by individual models
â”œâ”€â”€ utils/                     # Support scripts
â”‚   â”œâ”€â”€ download_dataset.py    # Download and pre-processing of MS MARCO
â”‚   â””â”€â”€ download_models.py     # Download model weights from HF
â”œâ”€â”€ .env                       # File for secret environment variables (e.g., HF_TOKEN)
â”œâ”€â”€ benchmark.py               # Answer generation script over RAG
â”œâ”€â”€ benchmark_summary.csv      # Final output with aggregated metrics table
â”œâ”€â”€ config.yaml                # Experiment orchestration file
â”œâ”€â”€ constants.py               # Global paths definition
â”œâ”€â”€ evaluate.py                # Metrics calculation script (ROUGE, BLEU, MRR, Sim)
â””â”€â”€ main.py                    # Main orchestrator (runs everything reading config.yaml)

```

---

## ğŸš€ Getting Started (Setup)


### 1. Environment Preparation

This project relies on specific library versions (e.g., PyTorch 2.2.0 and Transformers 4.45.2) to ensure compatibility with HPC clusters and specific model architectures. 

We highly recommend using **Conda** to manage dependencies, as it natively handles complex C++ libraries like FAISS much better than standard pip.

You can automatically recreate the exact environment using the provided `environment.yml` file. Run the following commands in your terminal at the root of the project:

```bash
# Create the conda environment from the yaml file
conda env create -f environment.yml

# Activate the newly created environment
conda activate rag-benchmark
```

### 2. Secrets Configuration (`.env`)

To download certain gated models (like Llama-3 or Gemma-2), a Hugging Face token is required.

1. Copy or rename the `.env.example` file to `.env`:
```bash
cp .env.example .env

```


2. Insert your token inside the `.env` file:
```env
HF_TOKEN=hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

```



*(Note: The `.env` file is ignored by Git for security reasons).*

---

## âš™ï¸ Configuration: `constants.py` vs `config.yaml`

The project uses two distinct configuration files. Understanding when to use which is crucial:

* ğŸ› ï¸ **`constants.py` (System Paths):** Modify this file **only the first time** if you want to change the project's folder structure (e.g., if you prefer to save data on an external hard drive or an HPC *scratch* partition). It contains constants like `DATA_PATH`, `MODELS_DIR`, etc.
* ğŸ§  **`config.yaml` (Experiment Brain):** Modify this file **before every run**. This is where you define the experiment: which models to download, which to test in the benchmark, how many questions to process (`limit`), and which pipeline phases to skip (via the `true/false` flags under `pipeline:`).

---

## ğŸƒâ€â™‚ï¸ Running the Benchmark

### The Recommended Method (YAML Orchestrator)

The best and most reproducible way to launch the benchmark (especially on clusters) is to use the `main.py` script, which will automatically read your configuration from `config.yaml`.

1. Open `config.yaml` and set the models and phases to execute (e.g., set only `run_benchmark` and `run_evaluation` to `true`).
2. Start the pipeline:
```bash
python main.py --config config.yaml

```



*(If working on a cluster, run this command inside your submission script `run-benchmark.sh`).*

---

<details>
<summary><strong>ğŸ› ï¸ Manual Execution (Single Scripts)</strong></summary>


If you need to debug or want to run individual modules step-by-step by passing command-line parameters, you can use the dedicated scripts directly:

**0. Dataset Download and Pre-processing:**

```bash
python utils/download_dataset.py

```

**1. Models Download:**

```bash
python utils/download_models.py --models llama-3.2-1b qwen-2.5-3b --token YOUR_TOKEN_HERE

```

**2. FAISS Ingestion:**
*(This operation is integrated into the data preparation, but if you scripted `ingestion.py` separately, you can call it here).*

**3. Benchmark Start (Generation):**

```bash
python benchmark.py --models llama-3.2-1b qwen-2.5-3b --limit 5

```

*(This will create the JSON files inside the `results/` folder).*

**4. Metrics Calculation:**

```bash
python evaluate.py --dir results --output benchmark_summary.csv

```

</details>

---

## ğŸ“Š Results

Upon completion, you will find:

* The raw files generated by each model in `results/results_MODEL-NAME.json`.
* The summary table of all models in the `benchmark_summary.csv` file, ready to be imported into Excel or Pandas for charts and analysis.
